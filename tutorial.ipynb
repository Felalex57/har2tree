{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Har2Tree Tutorial\n",
    "\n",
    "Crawling a web page can sound like a bit of an abstract concept at first. How exactly can we extract data from a web page? What data is really interesting to look at? Where can it be found?  \n",
    "\n",
    "&rarr; Every web browser generates a **[HAR file](https://www.keycdn.com/support/what-is-a-har-file#:~:text=HAR%2C%20short%20for%20HTTP%20Archive,times%2C%20and%20page%20rendering%20problems.)** (short for http archive) when loading a web page. This file mostly contains information about what resources are loaded by the browser, as it was firstly designed to identify possible performance issues. However, as the whole file is in a *standard JSON* format, **we can reverse engineer the process to extract useful information** and make a whole tree out of all the resources found in that HAR file. This step is particularly important as it is really complicated to understand what is going on by simply looking at the HAR file. *Example [here](https://gist.githubusercontent.com/Felalex57/8a90a3bd0628e3aef16ee04fb08e7e7e/raw/ecee33d26c5696989c600ba87683becff270ccc1/example.har)!*\n",
    "\n",
    "This notebook will guide you through the core features that **[Har2Tree](https://github.com/Lookyloo/har2tree)** offers.\n",
    "\n",
    "It is also important to note that Har2Tree is an API based on the **[TreeNode](http://etetoolkit.org/docs/latest/reference/reference_tree.html) class of ETE3 Toolkit** and that a lot of help can be found on the documentation there in case you want to know a bit more about how the program works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before we do anything: Setup\n",
    "\n",
    "## 1. Prerequisites\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following tutorial, we assume you have the following environment at your disposal.\n",
    "\n",
    "1. Ubuntu 20.04 or 20.10. You can also work with WSL 2 \n",
    "\n",
    "\n",
    "2. Python 3.8 or 3.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Installing har2tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are here it means that you already cloned the har2tree repository:  **you should be all set up already**!\n",
    "\n",
    "In case you got here another way, simply clone the repository in your desired folder:  \n",
    "```bash\n",
    "git clone https://github.com/Lookyloo/har2tree.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) 3. Retrieving useful files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you could use a pre-existing capture made for the tests of har2tree. They are located in **`tests / capture_samples`**.  \n",
    "However, you might want to take a look at how the files are downloaded **to have a better understanding of the program** and eventually use it on some pages of your choice.\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "**Important note:** Because Har2Tree was made for Lookyloo, it may require some additional files located in the same folder as the HAR file to be completely operational. To ensure that the program will fully work, we will simulate a capture using the **[public Lookyloo instance](https://lookyloo.circl.lu/)** rather than download the HAR file in the conventional way *(on Chrome: Ctrl + Shift + J > Network > F5 (Reload the page) > Arrow facing downwards)* . \n",
    "\n",
    "\n",
    "\n",
    "By simply **adding `/export` at the end of the url** when browsing on a capture, we can **download all the files generated by Lookyloo**. This includes the complete html capture of the page along with various other files that we will get into later on.\n",
    "\n",
    " \n",
    "Capture link: &nbsp;&nbsp; &rarr;  https://lookyloo.circl.lu/tree/b6b29698-4c97-4a21-adaa-f934e5bfb042  \n",
    "Download link: &rarr; https://lookyloo.circl.lu/tree/b6b29698-4c97-4a21-adaa-f934e5bfb042/export\n",
    "\n",
    "You can then unzip the folder in the desired folder of your choice and your HAR folder is now ready!  \n",
    "**Tip:** unzip the folder in the same directory as this notebook, it will be easier for later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The place where the magic of the API begins is the **[CrawledTree object](https://github.com/Lookyloo/har2tree/blob/9f92dab3909e406877cb36b3dbc30d0c5ead8c63/har2tree/parser.py#L15) :**  it takes a list of **HAR file paths** and a  **[uuid](https://en.wikipedia.org/wiki/Universally_unique_identifier#:~:text=A%20universally%20unique%20identifier%20UUID,%2C%20for%20practical%20purposes%2C%20unique.)** as parameters. <br/> To keep things simple for now, we will only be using **one HAR file per tree**.\n",
    "To build OS paths in python, we are going to use the **Path** class from **pathlib**.  \n",
    "\n",
    "Note that the keyword `__file__` doesn't work on Jupyter.  \n",
    "\n",
    "Let's see how we can tell the program to display our home directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path.home()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now let's try to create our first tree. As mentioned before, you will also need to pass a uuid as a parameter, but don't worry, python has everything you need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "uuid.uuid4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Little notes though:\n",
    "- *CrawledTree* takes a string as parameter and not a UUID, we just have to make a little conversion\n",
    "- it takes a list of HAR paths, even if there's only one path as mentioned before\n",
    "\n",
    "You might want to change the HAR path to what you downloaded in part 3 of the setup.\n",
    "Enough talking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from har2tree import CrawledTree\n",
    "har_path = Path.home() / 'har2tree' / 'tests' / 'capture_samples' / 'http_redirect' / '0.har'\n",
    "my_first_crawled_tree = CrawledTree([har_path], str(uuid.uuid4()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you didnt get an error, everything worked! Let's now see what we can do with that CrawledTree. \n",
    "You can find all the **properties** in the **[parser.py](https://github.com/Lookyloo/har2tree/blob/9f92dab3909e406877cb36b3dbc30d0c5ead8c63/har2tree/parser.py#L76)** file.\n",
    "\n",
    "First, let's see what website you got the capture from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_first_crawled_tree.root_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why not also check at what time the capture was made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_first_crawled_tree.start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
